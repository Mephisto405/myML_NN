[A1-Q1]
gradient descent optimization -> gradient descent는 비탈길로 공이 굴러가는 것과 같다.
공은 굴러가다 minimum에서 멈출 것이다. 그러나 이것이 local min인지 grobal min인지는 알기 힘들다.
momentum update and nestrov's update가 이 문제점을 어느 정도 해결해줄 수 있다.
가속도를 부여하여 local min (작은 웅덩이) 정도는 탈출할 수 있게끔 하는 것이다.
이 아이디어를 좀 더 확장해보자. 어떻게 하면 작은 웅덩이를 쉽게 빠져나갈 수 있을까.
직관적으로, (공기저항을 무시할 때) 경사가 가파르거나(속력이 빠르다) 웅덩이가 작거나 공이 클수록 관성에 의해 쉽게 빠져나갈 것이다.
경사나 웅덩이의 크기는 우리가 결정할 수 없는 요소이니 사고에서 제외하자.
공이 크다는 것은 어떤 의미일까? 어떻게 이 사실을 gradient descent optimization에 이용할 수 있을까?

[A2-Q2]
sgd의 고질적 단점은 loss의 그래프가 매우 들쭉날쭉하다는 것이다.
이는 dataset의 local한 부분(mini-batch)으로만 gradient를 계산하기 때문이다.
이렇게 되면 오히려 지금이 과거보다 더 train accuracy나 validation accuracy가 떨어질 수 있다.
보완할 방법 없을까?
과거 10(또는 50이나 100)개의 {W: Loss} key-value map을 기억하자.
이 10개의 loss를 어떤 메커니즘에 따라 비교해보자.
loss가 점차 줄어드는 것이 아니라 들쭉날쭉하다면 10개의 W를 평균 내버리고 learning rate를 반으로 줄이는 건 어떨까?
직관적으로, W의 convex hull에 (local) minimum이 있을 것 같다.
만약 이걸 구현했을 때 좋은 성능을 보여준다면, 좀 더 발전된 아이디어도 생각할 수 있다.
단순하게 평균을 구하는 것이 아니라, 어떤 메커니즘에 따라 가중치 평균을 구하는 것이다.
가중치는 loss들의 상대적 크기, 제곱 크기, 제곱근 크기 등으로 생각해볼 만하다.
